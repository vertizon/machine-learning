{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习概念的定义\n",
    "\n",
    "机器学习算法的定义：\n",
    "\n",
    "对于某类任务$T$和性能度量$P$，一个计算机程序被认为可以从经验$E$中学习是指，通过经验$E$改进后，它在任务$T$上由性能度量$P$衡量的性能有所提升。\n",
    "\n",
    "解析：\n",
    "\n",
    "- 任务$T$：\n",
    "    - 输入是样本，输出是值。\n",
    "    - 样本是数量化的特征的集合。\n",
    "    - 值可能是预测的分类、预测的值等。\n",
    "    - 任务的分类可能是分类、回归、转录、合成等。\n",
    "- 性能度量$P$\n",
    "    - 任务的准确率等，根据任务而设的性能度量。\n",
    "- 经验$E$\n",
    "    - 经验即数据。\n",
    "    - 数据有带标签的、有没有带标签的。有一半数据带标签的。有从环境中取数据的。\n",
    "- 算法\n",
    "    - 从输入到输出的计算过程。\n",
    "- 两个算法\n",
    "    - 算法1：**推理（inference）**，完成任务的模型\n",
    "    - 算法2：**训练（train)**，输入是数据、模型，输出是模型，这是机器学习关注的重点\n",
    "    \n",
    "- 机器学习中训练算法（算法2）的目标：如何训练出性能度量$P$更好的模型（算法1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "训练即机器学习中的学习算法，输入数据和模型，输出模型。输出的模型和输入的模型的参数是不一样的。训练的过程：\n",
    "\n",
    "- 明确任务\n",
    "- **数据**：收集、加载、探索、预处理数据，将数据划分为训练集、验证集和测试集\n",
    "- 构建模型\n",
    "- 确定优化任务的目标函数\n",
    "- 使用**梯度下降**进行优化，得到模型参数\n",
    "- 在验证集上验证和调整参数\n",
    "- 在测试集上评估性能度量$P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理\n",
    "\n",
    "数据样本输入模型，输出模型的输出值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "\n",
    "划分:\n",
    "- 训练集：用来训练模型，目标是降低训练误差\n",
    "- 测试集：用来评估模型的泛化误差\n",
    "    - 泛化：在先前未观测到的输入上表现良好的能力\n",
    "    - 泛化误差：也称测试误差，在未观测到的输入上的误差的期望\n",
    "- 验证集\n",
    "    - 评估泛化误差，用来选择超参数\n",
    "\n",
    "独立同分布假设\n",
    "- 数据集中的每个样本都是彼此相互独立的，并且训练集和测试集是同分布的，采样自相同的分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "优化的定义：改变$x$以最小化某个函数$f(x)$的任务。（$f(x)$被称为目标函数）\n",
    "\n",
    "梯度下降是一种优化方法，定义为：$x$沿着梯度的反方向移动一小步，使$f(x)$得值下降\n",
    "\n",
    "解析：\n",
    "- 方向导数：某方向的斜率\n",
    "- 梯度：梯度是偏导的方向。在梯度方向，方向导数最大。因此，沿着梯度的反方向移动$x$，就是$f(x)$下降最大的方向\n",
    "\n",
    "### 随机梯度下降\n",
    "\n",
    "定义：将训练集划分为大小为$m$的minibatch，每次在minibatch上计算目标函数的梯度，作为在训练集总体上的梯度的近似值，并用它更新目标函数的参数。（前提：目标函数可以分解为每个样本的代价的和，通常情况下这一点是满足的）\n",
    "\n",
    "用途：\n",
    "- 当训练集很大时，在所有训练集上计算梯度代价大。\n",
    "- 在minibatch上求梯度，代替在整个数据集上求梯度，可行性依据：梯度是期望，期望可以用小规模样本近似。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的容量、欠拟合、过拟合\n",
    "\n",
    "容量：模型拟合各种函数的能力。\n",
    "\n",
    "欠拟合：模型不能在训练集上获得足够低的误差。\n",
    "\n",
    "过拟合：模型在训练集上误差低，但在测试集上误差大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化 (regularization)\n",
    "\n",
    "正则化的定义：减少泛化误差的策略。（只要能减少泛化误差的策略，都可以叫正则化）\n",
    "\n",
    "正则化策略有如下：\n",
    "- 对参数用范数进行惩罚\n",
    "    - L2范数\n",
    "    - L1范数\n",
    "\n",
    "### 对参数用范数进行惩罚\n",
    "\n",
    "定义：$\\tilde{J}(\\theta;X,y) = J(\\theta;X,y) + \\alpha \\Omega (\\theta) $，添加参数的惩罚项$\\alpha \\Omega (\\theta)$，限制模型的学习能力。\n",
    "\n",
    "解析：\n",
    "- 当我们的训练算法最小化正则化后的目标函数$\\tilde{J}$ 时，它会降低原始目标$J$关于训练数据的误差并同时减小在某些衡量标准下参数$\\theta$（或参数子集）的规模。选择不同的参数范数$\\Omega$会偏好不同的解。\n",
    "- 通常只对权重做惩罚，而不对偏置做惩罚。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
